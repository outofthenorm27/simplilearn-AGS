{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "CHNX4Tn4cPb1",
   "metadata": {
    "id": "CHNX4Tn4cPb1"
   },
   "source": [
    "#**Demo: Text Generation**\n",
    "\n",
    "This demonstration employs the Natural Language Toolkit (NLTK) and the Brown corpus to demonstrate text generation through a Markov chain model using trigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k-2DSxXRc1gn",
   "metadata": {
    "id": "k-2DSxXRc1gn"
   },
   "source": [
    "##**Steps to Perform:**\n",
    "\n",
    "Step 1: Import the Necessary Libraries\n",
    "\n",
    "Step 2: Define Stopwords and Punctuation\n",
    "\n",
    "Step 3: Load Sentences and Generate N-grams\n",
    "\n",
    "Step 4: Remove Stopwords from N-grams\n",
    "\n",
    "Step 5: Calculate Frequency Distributions\n",
    "\n",
    "Step 6: Create a Dictionary of Trigram Frequencies\n",
    "\n",
    "Step 7: Define the Text Generation Function\n",
    "\n",
    "Step 8: Execute the Text Generation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F77eQldddrgN",
   "metadata": {
    "id": "F77eQldddrgN"
   },
   "source": [
    "###**Step 1: Import the Necessary Libraries**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   Import the necessary libraries and set up the OpenAI API key.\n",
    "*   Download the necessary NLTK packages and corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PgPYiaZsc-3O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PgPYiaZsc-3O",
    "outputId": "537d95f6-a6b4-4832-eb0c-b12618f5c381"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download necessary NLTK packages and corpus\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QuDAtVJAeKPZ",
   "metadata": {
    "id": "QuDAtVJAeKPZ"
   },
   "source": [
    "###**Step 2: Define Stopwords and Punctuation**\n",
    "\n",
    "*   Stopwords are common words in a language that are often considered to be of little value in text analysis.\n",
    "*   Punctuation refers to characters used to separate sentences, clauses, phrases, or words in writing.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2MHWS4tUeZYO",
   "metadata": {
    "id": "2MHWS4tUeZYO"
   },
   "outputs": [],
   "source": [
    "# Define stopwords and punctuation\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "string.punctuation += '\"\\'-â€”'\n",
    "removal_list = list(stop_words) + list(string.punctuation) + ['lt', 'rt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BD7fr4rheeOW",
   "metadata": {
    "id": "BD7fr4rheeOW"
   },
   "source": [
    "###**Step 3: Load Sentences and Generate N-grams**\n",
    "\n",
    "*   Load sentences from the Brown corpus and generate N-grams.\n",
    "*   By the end of this process, **unigram**, **bigram**, and **trigram** lists will contain the respective N-grams for the sentences in the Brown corpus.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pTKPSbw_elM2",
   "metadata": {
    "id": "pTKPSbw_elM2"
   },
   "outputs": [],
   "source": [
    "# Load sentences from the Brown corpus\n",
    "sents = brown.sents()\n",
    "\n",
    "# Initialize lists for storing n-grams\n",
    "unigram = []\n",
    "bigram = []\n",
    "trigram = []\n",
    "\n",
    "# Generate n-grams\n",
    "for sentence in sents:\n",
    "    sentence = [word.lower() for word in sentence if word not in string.punctuation]\n",
    "    unigram.extend(sentence)\n",
    "    bigram.extend(list(ngrams(sentence, 2, pad_left=True, pad_right=True)))\n",
    "    trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oXbe4Qgpepzk",
   "metadata": {
    "id": "oXbe4Qgpepzk"
   },
   "source": [
    "###**Step 4: Remove Stopwords from N-grams**\n",
    "\n",
    "*   Define a function to remove stopwords from the N-grams.\n",
    "*   Use it to clean the bigrams and trigrams.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6VHX-QXhe86T",
   "metadata": {
    "id": "6VHX-QXhe86T"
   },
   "outputs": [],
   "source": [
    "# Function to remove stopwords from n-grams\n",
    "def remove_stopwords(ngrams, n):\n",
    "    if n == 2:\n",
    "        return [(a, b) for (a, b) in ngrams if a not in removal_list and b not in removal_list]\n",
    "    elif n == 3:\n",
    "        return [(a, b, c) for (a, b, c) in ngrams if a not in removal_list and b not in removal_list and c not in removal_list]\n",
    "\n",
    "# Remove stopwords from n-grams\n",
    "bigram = remove_stopwords(bigram, 2)\n",
    "trigram = remove_stopwords(trigram, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FLLxqiIufGp-",
   "metadata": {
    "id": "FLLxqiIufGp-"
   },
   "source": [
    "###**Step 5: Calculate Frequency Distributions**\n",
    "\n",
    "*   Calculate the frequency distributions of the bigrams and trigrams.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zmq_1nIWfNg4",
   "metadata": {
    "id": "Zmq_1nIWfNg4"
   },
   "outputs": [],
   "source": [
    "# Calculate frequency distributions\n",
    "freq_bi = FreqDist(bigram)\n",
    "freq_tri = FreqDist(trigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sB-t5lnjfP_V",
   "metadata": {
    "id": "sB-t5lnjfP_V"
   },
   "source": [
    "###**Step 6: Create a Dictionary of Trigram Frequencies**\n",
    "\n",
    "*   Create a dictionary of trigram frequencies to use it in the text generation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DyEVQsOxfaUq",
   "metadata": {
    "id": "DyEVQsOxfaUq"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of trigram frequencies\n",
    "d = defaultdict(Counter)\n",
    "for ngram in freq_tri:\n",
    "    if None not in ngram:\n",
    "        d[ngram[:-1]][ngram[-1]] += freq_tri[ngram]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5tcWjGz8fd4l",
   "metadata": {
    "id": "5tcWjGz8fd4l"
   },
   "source": [
    "###**Step 7: Define the Text Generation Function**\n",
    "\n",
    "*   Define the **generate_text** function to generate text based on the trigram frequencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45E4k0DEfnZM",
   "metadata": {
    "id": "45E4k0DEfnZM"
   },
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(prefix, n=20):\n",
    "    for _ in range(n):\n",
    "        suffix_candidates = list(d.get(prefix, Counter()).elements())\n",
    "        if not suffix_candidates:\n",
    "            new_prefix = random.choice(unigram), random.choice(unigram)\n",
    "            yield new_prefix[0]  # Yield the first word of the new prefix\n",
    "            prefix = new_prefix\n",
    "        else:\n",
    "            suffix = random.choice(suffix_candidates)\n",
    "            yield suffix\n",
    "            prefix = (*prefix[1:], suffix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vdBJRj6AftiC",
   "metadata": {
    "id": "vdBJRj6AftiC"
   },
   "source": [
    "###**Step 8: Execute the Text Generation Function**\n",
    "\n",
    "*   Call the **generate_text** function and print the generated text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2OXd5-fVgDJX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2OXd5-fVgDJX",
    "outputId": "7c8cc821-c459-42ca-a22f-49c05ee1630a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plague blue mortgage of not is tonight '' even women other glowed larger joined the the himself to of young\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "prefix = (\"he\", \"said\")\n",
    "generated_text = list(generate_text(prefix))\n",
    "if generated_text:\n",
    "    print(\" \".join(generated_text))\n",
    "else:\n",
    "    print(\"No text generated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jozwTLaJUxPo",
   "metadata": {
    "id": "jozwTLaJUxPo"
   },
   "source": [
    "##**Conclusion**\n",
    "\n",
    "This demo showcases NLTK and the Brown corpus for trigram-based Markov chain text generation. Users can run it multiple times to observe the varying generated outputs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
